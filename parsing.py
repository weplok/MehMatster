import requests
from bs4 import BeautifulSoup


# –ø–∞—Ä—Å–∏–º —Å–æ–±—ã—Ç–∏—è –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞
def student_get_news():
    headers = {
        "user": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
    }

    url = "https://sfedu.ru/"
    page = requests.get(url=url, headers=headers)

    soup = BeautifulSoup(page.text, "html.parser")
    all_student_news = soup.findAll('div', class_='index-news-list__wrapper')
    # print(all_student_news)
    title = soup.find('div', class_='h3')
    link_class = soup.find('li', class_='index-news-list__more')
    link = link_class.findAll('a')[0]['href']
    filter_student_news = []
    for data in all_student_news:
        if data.find('li') is not None:
            # print(data.text)
            filter_student_news.append(data.text)
    # filter_student_news.append(link)
    filter_student_news = ''.join(filter_student_news)
    filter_student_news = filter_student_news.split('\n')
    res_student_news = []
    for new in filter_student_news:
        if new != '' and new != '–ë–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π':
             res_student_news.append('üëÄ ->' + new + '\n\n')

    print(res_student_news)
    return ''.join(res_student_news)+'–ß—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π, –Ω–∞–∂–∏–º–∞–π –Ω–∞ –ü—Ä–µ—Å—Å-–¶–µ–Ω—Ç—Ä –Æ–§–£ üëÜ'


def student_get_news_mehmat():
    headers = {
        "user": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
    }

    url = "https://mmcs.sfedu.ru/"
    page = requests.get(url=url, headers=headers)

    soup = BeautifulSoup(page.text, "html.parser")
    news_cards = soup.findAll('div', class_='news_item_f')
    # print(news_cards)
    filter_news_cards = []
    for data in news_cards:
        # print(data)
        # print('------------')
        data_title = data.find('h2', class_='article_title')
        data_date = data.find('div', class_='newsitem_tools')
        data_main_info = data.find('div', class_='newsitem_text')
        if data_title is not None and data_date is not None and data_main_info is not None:
            filter_news_cards.append('üìù' + (''.join(data_title.text.split('\n'))) + '\n' + 'üïë' + ''.join(data_date.text.split('\n'))
                                     + '\n' + ''.join(data_main_info.text.split('\n')) + '\n\n\n')
    return "".join(filter_news_cards)


# —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—é–¥–∂–µ—Ç–Ω—ã—Ö –º–µ—Å—Ç –∏—Å—Ö–æ–¥—è –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
def applicant_count_free_places(direction):
    url = "https://sfedu.ru/www/stat_pages22.show?p=ABT/N8206"
    page = requests.get(url=url)

    soup = BeautifulSoup(page.text, "html.parser")
    mim_dir_row = soup.find('tr', class_='row2')
    mim_dir_quantity = mim_dir_row.find('td', class_='column3 style10 n').text

    pmi_dir_row = soup.find('tr', class_='row3')
    pmi_dir_quantity = pmi_dir_row.find('td', class_='column3 style10 n').text

    fiit_dir_row = soup.find('tr', class_='row4')
    fiit_dir_quantity = fiit_dir_row.find('td', class_='column3 style10 n').text

    moais_dir_row = soup.find('tr', class_='row7')
    moais_dir_quantity = moais_dir_row.find('td', class_='column3 style10 n').text

    pi_dir_row = soup.find('tr', class_='row19')
    pi_dir_quantity = pi_dir_row.find('td', class_='column3 style10 n').text

    ivt_dir_row = soup.find('tr', class_='row28')
    ivt_dir_quantity = ivt_dir_row.find('td', class_='column3 style10 n').text

    free_places = {
        '–ø—Ä–∏–∫–ª–∞–¥–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞': pmi_dir_quantity,
        '—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': fiit_dir_quantity,
        '–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞': ivt_dir_quantity,
        "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –º–µ—Ö–∞–Ω–∏–∫–∞": mim_dir_quantity,
        '–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º': moais_dir_quantity,
        '–ø—Ä–∏–∫–ª–∞–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞': pi_dir_quantity
    }

    return free_places[direction.lower()]



# —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–ª–∞—Ç–Ω—ã—Ö –º–µ—Å—Ç –∏—Å—Ö–æ–¥—è –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
def applicant_count_paid_places(direction):
    url = "https://sfedu.ru/www/stat_pages22.show?p=ABT/N8206"
    page = requests.get(url=url)

    soup = BeautifulSoup(page.text, "html.parser")
    mim_dir_row = soup.find('tr', class_='row2')
    mim_dir_quantity = mim_dir_row.find('td', class_='column7 style10 n').text

    pmi_dir_row = soup.find('tr', class_='row3')
    pmi_dir_quantity = pmi_dir_row.find('td', class_='column7 style10 n').text

    fiit_dir_row = soup.find('tr', class_='row4')
    fiit_dir_quantity = fiit_dir_row.find('td', class_='column7 style10 n').text

    moais_dir_row = soup.find('tr', class_='row7')
    moais_dir_quantity = moais_dir_row.find('td', class_='column7 style10 n').text

    pi_dir_row = soup.find('tr', class_='row19')
    pi_dir_quantity = pi_dir_row.find('td', class_='column7 style10 n').text

    ivt_dir_row = soup.find('tr', class_='row28')
    ivt_dir_quantity = ivt_dir_row.find('td', class_='column7 style10 n').text

    free_places = {
        '–ø—Ä–∏–∫–ª–∞–¥–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞': pmi_dir_quantity,
        '—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': fiit_dir_quantity,
        '–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞': ivt_dir_quantity,
        "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –º–µ—Ö–∞–Ω–∏–∫–∞": mim_dir_quantity,
        '–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º': moais_dir_quantity,
        '–ø—Ä–∏–∫–ª–∞–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞': pi_dir_quantity
    }

    return free_places[direction.lower()]









